{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "915c08b5-b64d-4d43-97e5-e895faa6508b",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "\n",
    "In my journey to explore the realm of data science, I embarked on a project aimed at mastering key modules such as Matplotlib, scikit-learn, and Pandas. Through this project, I endeavored to develop not only technical proficiency but also the ability to craft compelling narratives from data.\n",
    "\n",
    "With this in mind, I got acquainted with the modules through the following task. I took on the task of plotting how my academic performance has evolved over time, leveraging data visualization techniques to enable meaningful comparisons. I also sought to create a model that can \"predict\" what my academic performance will be in the future. As this was my first time seriously working with these modules, I used a straightforward linear regression model for this purporse. I would like to clarify that this model was created to \"have a little fun\" and was not meant as a \"serious model\". For a meaningful model, one would need to divide the data into test (20% is the industry standard) and training (80%) sets and evaluate the model afterwards. I will attempt this in the future on another dataset with better explanatory variables. Overall, I achieved my goal of getting acquainted with the modules and telling a story using data.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "As of writing this, I have not provided a test pdf to use this notebook with, but I plan on doing so in the future.\n",
    "\n",
    "If you are an RWTH student and would like to use this notebook for your own grades, you can readily do so. Simply download your report card \"Notenspiegel(Alles)\" and save it in the same folder as this file under \"Notenspiegel.pdf\" (Alternatively you can edit the file_path variable to the path of the document).\n",
    "\n",
    "### Other dependencies\n",
    "\n",
    "This notebook uses the following. Please ensure you have Python and Java installed. When writing the execution instructions, I ke used VS code when writing down the execution instuctions, so if you are not familiar with venvs kindly download it, it provides a very easy way to create them without having to use the command line.\n",
    "\n",
    "The remaining dependencies will be automatically installed, when you create the venv.\n",
    "\n",
    "- [Python](https://www.python.org/) (version 3.12.2 or later)\n",
    "- [Java](https://www.java.com/) (version 11.0.13 or later)\n",
    "- [Pandas](https://pandas.pydata.org/) (version 2.2.1 or later)\n",
    "- [Matplotlib](https://matplotlib.org/) (version 3.8.0 or later)\n",
    "- [scikit-learn](https://scikit-learn.org/stable/) (version 1.3.0 or later)\n",
    "- [tabula-py](https://github.com/chezou/tabula-py) (version 2.7.0 or later)\n",
    "- [Ipython Kernel](https://ipython.org/) (version 6.29.3 or later)\n",
    "- [jpype](https://github.com/jpype-project/jpype) (version 1.5.0 or later)\n",
    "\n",
    "## Setting up a venv (in VS Code)\n",
    "\n",
    "At this point, you should have already installed Python and java. Restart VS Code before proceeding (if you had already opened it).\n",
    "\n",
    "### What is a venv (Virtual environment)?\n",
    "\n",
    "Virtual environments are lightweight environments existing on top of the base Python installation, allowing different sets of packages to be installed in different directories. Why is this useful? Assume that you have a module that you use in some code that you wrote. It happens to be deprecated, but you haven't gotten around to updating your code. If you were to run this notebook, it may upgrade said depency to a newer verion and thus cause your code to break. By using venvs the required packages remain isolated from each other. \n",
    "\n",
    "While writing this, I used a conda environment, which is an analogous environment that is handled by the anaconda environment and package manager. Due to the fact that it would be another required install and it is not necessary, I listed the instructions for configuring a venv instead.\n",
    "\n",
    "Before starting, we need to install the required modules. Open VS Code and navigate to the folder that contains this file, there you will also find requirements.txt document. When you open the folder in VS Code, you will need to select a kernel as to run this notebook. Click  \"Select kernel\" and then \"Select another kernel\". Create a new python environment, a venv, with Python 3.12 as the interpreter. Select requirements.txt and then click OK when it asks you which dependencies to install. Once the venv is created, you are ready to roll. Restart VS Code and everything should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0827afcd-9a7e-45d9-9e86-abad35040477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tabula\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "tabula.environment_info()\n",
    "#Let the file of your report card be in the same directory as this file, and have the title \"Notenspiegel.pdf\".\n",
    "#Alternatively, you could have the file stored anywhere in your system and replace the bottom string with \n",
    "#the corresponding file path.\n",
    "file_path = \"Notenspiegel.pdf\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae9b2fd",
   "metadata": {},
   "source": [
    "## Fetching the Data from the PDF and Creating the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128a0c7e-6132-4b2d-8bba-f360fbf139c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = tabula.read_pdf(file_path, pages='all', encoding=\"utf-8\")\n",
    "\n",
    "for df in dfs:\n",
    "        for c in df:\n",
    "            try:\n",
    "                df[c] = pd.to_numeric(df[c])\n",
    "            except ValueError:\n",
    "                # We need to catch errors explicitly as 'ignore' will cause errors in future versions\n",
    "                pass \n",
    "\n",
    "#We get rid of the duplicate (English) entries \n",
    "dfs = dfs[:len(dfs)//2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3582f35b-10c6-463e-9978-cc4fbf969762",
   "metadata": {},
   "source": [
    "We have succesfuly imported the tables, each table is imported as a DataFrame, so lets merge them into one instead and take a look at a snippet of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93266823-c7cd-48d3-9b1a-afc0ce8d7ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "grades_df =  pd.concat(dfs, axis=0)\n",
    "print(grades_df.shape)\n",
    "grades_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6d752e-82b2-4527-89d7-5e42ae008885",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "We need to clean our data before we can use it, there are many entries that can be ignored e.g. entries that are cummulative, like the first entry in our data frame. We will delete these entries and remain with our cleaned data set. We will also drop the exams that were not passed (returned 0 credits) or for which one did not receive a grade for (0 in the column Note).\n",
    "\n",
    "We will also transform the columns to sensible datatypes. Using the correct datatypes such as the NumPy datetime64 for the column \"Datum\" makes dealing with dates much easier, as there are a lot of features that pandas has already implemented for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07804ce5-9d64-478c-8258-91de78c4bc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "grades_df.dtypes\n",
    "\n",
    "# Clean the data, remove NaN rows, transform data into the appropriate datatype\n",
    "grades_df['Datum'] = pd.to_datetime(grades_df['Datum'], dayfirst=True)\n",
    "grades_df['Note'] = grades_df['Note'].replace('B', 0)\n",
    "grades_df['Note'] = grades_df['Note'].apply(lambda x: str(x).replace(',', '.'))\n",
    "grades_df['CP'] = grades_df['CP'].apply(lambda x: str(x).replace(',', '.'))\n",
    "grades_df['Note'] = pd.to_numeric(grades_df['Note'], errors='coerce')\n",
    "grades_df['CP'] = pd.to_numeric(grades_df['CP'], errors='coerce')\n",
    "\n",
    "# Reset the index and drop NaN rows\n",
    "grades_df = grades_df[grades_df['Note']!=0]\n",
    "grades_df = grades_df[grades_df['CP']!=0]\n",
    "grades_df = grades_df.dropna()\n",
    "\n",
    "#We then sort the data by ascending Date.\n",
    "grades_df= grades_df.sort_values('Datum',ascending=True)\n",
    "grades_df = grades_df.reset_index(drop=True)\n",
    "grades_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d920d0a-8a41-41a1-aa8c-4e82e3718320",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Let's have a quick look at the features of our sorted data\n",
    "data_description = grades_df.describe()\n",
    "mean_grade = data_description['Note']['mean'].round(2)\n",
    "data_description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971c9c24-be4d-402e-8ceb-4726a67ad104",
   "metadata": {},
   "source": [
    "## Data Visualization and Linear Regression\n",
    "\n",
    "I really like that the df.describe()/Series.describe() method exists, with the information contained within it we can already create a boxplot. What is doubley handy, is that matplotlib already has an inbuilt function for this, and all we have to do is pass it the data and set our parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08625fe4-b696-452a-82da-2a58c31f29d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, ax1 = plt.subplots()\n",
    "ax1.set_title('Note Boxplot')\n",
    "ax1.boxplot(grades_df['Note'])\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.set_title('CP Boxplot')\n",
    "# Adjust whisker lengths to the 5th and 95th percentiles. If they were at the standard (1.5*IQR), it would look \n",
    "# less like a box plot as most of the data has the value 6 in my case. Feel free to adjust this to whatever suits your needs. \n",
    "# One has to note, that adjusting whiskers shouldn't be done hapharzadly and should always be explicitly documented\n",
    "# \n",
    "ax2.boxplot(grades_df['CP'], whis=[5, 95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd1afa9-0c35-46e8-bd10-8554ea2b194b",
   "metadata": {},
   "source": [
    "We would like to create a linear regression model as to predict future performance. We choose the month in which the exam was written and the days since the first exam ever written. We extract both of these from our data and create a model based on the entire training set.\n",
    "\n",
    "We then plug each date in our data set into the model and generate the plot \"Linear Regression\".\n",
    "\n",
    "We generate the days from the first exam until 2025 and plug them into the model, this gives us the plot \"Prediction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd8118e-1467-477c-8b5a-218bbb87a7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert dates to numeric representation (number of days since a reference date)\n",
    "grades_df['NumericDate'] = (grades_df['Datum'] - grades_df['Datum'].min()).dt.days\n",
    "\n",
    "# Extract month from dates\n",
    "grades_df['Month'] = grades_df['Datum'].dt.month\n",
    "\n",
    "# Prepare data\n",
    "X = grades_df[['NumericDate', 'Month']]\n",
    "y = grades_df['Note']\n",
    "\n",
    "# Fit linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict grades using the model\n",
    "predicted_grades = model.predict(X)\n",
    "\n",
    "# Extend dates into the future until 2025\n",
    "future_dates = pd.date_range(start=grades_df['Datum'].min(), end='2025-01-01')\n",
    "\n",
    "# Convert future dates to numeric representation\n",
    "future_numeric_dates = (future_dates - grades_df['Datum'].min()).days\n",
    "future_month = future_dates.month\n",
    "\n",
    "# Combine future numeric dates and month into a DataFrame with consistent feature names\n",
    "future_X = pd.DataFrame({'NumericDate': future_numeric_dates, 'Month': future_month})\n",
    "\n",
    "# Predict grades for future dates\n",
    "future_predicted_grades = model.predict(future_X)\n",
    "\n",
    "# Create a figure and axes\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot data as a scatter plot\n",
    "ax.scatter(grades_df['Datum'], grades_df['Note'], label='Data')\n",
    "\n",
    "# Plot regression line for existing data\n",
    "ax.plot(grades_df['Datum'], predicted_grades, color='red', label='Linear Regression')\n",
    "\n",
    "# Plot extension of regression line into the future\n",
    "ax.plot(future_dates, future_predicted_grades, color='blue', linestyle='--', label='Prediction')\n",
    "\n",
    "#Plot the mean grade as a horizontal line\n",
    "ax.axhline(mean_grade, color=\"red\", linestyle=\":\",label=\"mean\")\n",
    "ax.set_yticks([mean_grade,1,1.5,2,2.5,3,3.5,4])\n",
    "\n",
    "# Set date format to display only month and year\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%y'))\n",
    "\n",
    "# Set the y-axis range\n",
    "ax.set_ylim(0.9, 4)\n",
    "\n",
    "# Reverse the y-axis\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# Set title, labels and legend\n",
    "ax.set_title('Visualizing Academic Trends: Linear Regression and Predicted Performance')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Grade')\n",
    "ax.legend()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96368549-2d16-4709-97ef-d82c92b174a8",
   "metadata": {},
   "source": [
    "Let us have a final look at our data, we started of with 4 DataFrames and 7 Series and were left with 1 DataFrame and 9 Series. Thank you for reading through this; it was a fun project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242cd709-dbcb-4935-bafe-24db880702aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grades_df.shape)\n",
    "grades_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c4b226-0085-451f-9726-b89aba8e9bfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaef5c2-cbcf-4760-88e0-dae66ac8d437",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
